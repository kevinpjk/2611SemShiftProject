{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SemEval2020 Task 1 Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Train two models for two time points and align them using Orthogonal Procrustes\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Training word2vec model on english practice corpus 1\n",
    "model = Word2Vec(corpus_file=\"test_data_public/english/corpus1/lemma/ccoha1.txt\", vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "model.save(\"test_data_public/english/corpus1/lemma/ccoha1.model\")\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"test_data_public/english/corpus1/lemma/ccoha1.wv\")\n",
    "\n",
    "wv = KeyedVectors.load(\"test_data_public/english/corpus1/lemma/ccoha1.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train second word2vec model using corpus2 lines\n",
    "model2 = Word2Vec(corpus_file=\"test_data_public/english/corpus2/lemma/ccoha2.txt\", vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "model2.save(\"test_data_public/english/corpus2/lemma/ccoha2.model\")\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors2 = model2.wv\n",
    "word_vectors2.save(\"test_data_public/english/corpus2/lemma/ccoha2.wv\")\n",
    "\n",
    "wv2 = KeyedVectors.load(\"test_data_public/english/corpus2/lemma/ccoha2.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attack_nn', 'bag_nn', 'ball_nn', 'bit_nn', 'chairman_nn', 'circle_vb', 'contemplation_nn', 'donkey_nn', 'edge_nn', 'face_nn', 'fiction_nn', 'gas_nn', 'graft_nn', 'head_nn', 'land_nn', 'lane_nn', 'lass_nn', 'multitude_nn', 'ounce_nn', 'part_nn', 'pin_vb', 'plane_nn', 'player_nn', 'prop_nn', 'quilt_nn', 'rag_nn', 'record_nn', 'relationship_nn', 'risk_nn', 'savage_nn', 'stab_nn', 'stroke_vb', 'thump_nn', 'tip_vb', 'tree_nn', 'twist_nn', 'word_nn']\n"
     ]
    }
   ],
   "source": [
    "# Load target words from targets.txt\n",
    "target_words = []\n",
    "with open(\"test_data_public/english/targets.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        target_words.append(line.strip())\n",
    "\n",
    "print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align wv and wv2 using Orthogonal Procrustes\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# print(wv.get_normed_vectors(), wv2.get_normed_vectors())\n",
    "\n",
    "# Get vectors for target words\n",
    "wv_target_vectors = np.array([wv.get_vector(word) for word in target_words])\n",
    "wv2_target_vectors = np.array([wv2.get_vector(word) for word in target_words])\n",
    "\n",
    "wv_target_vectors_mu = wv_target_vectors.mean(axis=0)\n",
    "wv_target_vectors_centered = wv_target_vectors - wv_target_vectors_mu\n",
    "\n",
    "wv2_target_vectors_mu = wv2_target_vectors.mean(axis=0)\n",
    "wv2_target_vectors_centered = wv2_target_vectors - wv2_target_vectors_mu\n",
    "\n",
    "R, sca = orthogonal_procrustes(wv_target_vectors_centered, wv2_target_vectors_centered)\n",
    "# print(R, sca)\n",
    "scale = sca / np.square(norm(wv_target_vectors_centered))\n",
    "\n",
    "wv2_target_vectors_approx = scale * np.dot(wv_target_vectors_centered, R) + wv2_target_vectors_mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy (align):  0.5945945945945946\n"
     ]
    }
   ],
   "source": [
    "# Task 1 - Compute target words cosine similarity between aligned vectors and wv2 vectors\n",
    "task1_similarities = []\n",
    "for i, word in enumerate(target_words):\n",
    "    # print(word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1)))\n",
    "    task1_similarities.append([word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1))[0][0]])\n",
    "\n",
    "# print(task1_similarities)\n",
    "\n",
    "# Load truth file\n",
    "task1_truth = []\n",
    "with open(\"test_data_public/english/truth/binary.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        task1_truth.append(line.strip().split())\n",
    "\n",
    "# For each list in task1_similarities, check if value is less than 0.9, assign 1 if true, 0 if false\n",
    "task1_res = []\n",
    "for i, word in enumerate(task1_similarities):\n",
    "    if word[1] < 0.9:\n",
    "        task1_res.append([word[0], 1])\n",
    "    else:\n",
    "        task1_res.append([word[0], 0])\n",
    "\n",
    "# Compare task1_res and task1_truth\n",
    "task1_correct = 0\n",
    "for i, word in enumerate(task1_res):\n",
    "    if word[1] == int(task1_truth[i][1]):\n",
    "        task1_correct += 1\n",
    "\n",
    "print(\"Task 1 accuracy (align): \", task1_correct/len(task1_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Train a combined model on both corpora (target words in corpus2 are changed to target_word_)\n",
    "\n",
    "# Store corpus1 and corpus2 lines together\n",
    "c1_c2_lines = []\n",
    "\n",
    "with open(\"test_data_public/english/corpus1/lemma/ccoha1.txt\") as file:\n",
    "    c1_lines = [line.rstrip().split() for line in file]\n",
    "    c1_c2_lines.extend(c1_lines)\n",
    "\n",
    "with open(\"test_data_public/english/corpus2/lemma/ccoha2.txt\") as file:\n",
    "    c2_lines = [line.rstrip().split() for line in file]\n",
    "    for line in c2_lines:\n",
    "        for word in line:\n",
    "            if word in target_words:\n",
    "                line[line.index(word)] = word + '_'\n",
    "    c1_c2_lines.extend(c2_lines)\n",
    "\n",
    "# Train combined model\n",
    "model_combined = Word2Vec(c1_c2_lines, vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors_combined = model_combined.wv\n",
    "word_vectors_combined.save(\"test_data_public/english/ccoha_combined.wv\")\n",
    "\n",
    "wv_combined = KeyedVectors.load(\"test_data_public/english/ccoha_combined.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy (combined):  0.6756756756756757\n"
     ]
    }
   ],
   "source": [
    "# Task 1 - Compute target words cosine distance in combined word vector\n",
    "task1_similarities_combined = []\n",
    "for i, word in enumerate(target_words):\n",
    "    # print(word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1)))\n",
    "    task1_similarities_combined.append([word, cosine_similarity(wv_combined.get_vector(word).reshape(1,-1), wv_combined.get_vector(word + '_').reshape(1,-1))[0][0]])\n",
    "\n",
    "# print(task1_similarities_combined)\n",
    "\n",
    "# Compare task1_similarities_combined and task1_truth\n",
    "task1_combined_res = []\n",
    "for i, word in enumerate(task1_similarities_combined):\n",
    "    if word[1] < 0.65:\n",
    "        task1_combined_res.append([word[0], 1])\n",
    "    else:\n",
    "        task1_combined_res.append([word[0], 0])\n",
    "\n",
    "task1_combined_correct = 0\n",
    "for i, word in enumerate(task1_combined_res):\n",
    "    if word[1] == int(task1_truth[i][1]):\n",
    "        task1_combined_correct += 1\n",
    "\n",
    "print(\"Task 1 accuracy (combined): \", task1_combined_correct/len(task1_combined_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/cc726n6j1h36w_z2c__p9hk40000gp/T/ipykernel_33825/2787650697.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['word1'] = df['word1'].str.replace('[^a-zA-Z]', ' ').str.split()\n",
      "/var/folders/5c/cc726n6j1h36w_z2c__p9hk40000gp/T/ipykernel_33825/2787650697.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['word2'] = df['word2'].str.replace('[^a-zA-Z]', ' ').str.split()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>word1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>word2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5548</td>\n",
       "      <td>[pope]</td>\n",
       "      <td>→</td>\n",
       "      <td>[ruff, fish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6550</td>\n",
       "      <td>[hat]</td>\n",
       "      <td>→</td>\n",
       "      <td>[mushroom, cap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750</td>\n",
       "      <td>[to, search, to, look, for]</td>\n",
       "      <td>↔</td>\n",
       "      <td>[to, want]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4864</td>\n",
       "      <td>[heart]</td>\n",
       "      <td>→</td>\n",
       "      <td>[hearts, in, cards]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6858</td>\n",
       "      <td>[country]</td>\n",
       "      <td>→</td>\n",
       "      <td>[turkey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>509</td>\n",
       "      <td>[comb, of, a, bird]</td>\n",
       "      <td>—</td>\n",
       "      <td>[comb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>973</td>\n",
       "      <td>[to, stand, up]</td>\n",
       "      <td>→</td>\n",
       "      <td>[to, revolt, rebel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1303</td>\n",
       "      <td>[to, pull, to, draw]</td>\n",
       "      <td>→</td>\n",
       "      <td>[to, slow, linger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>751</td>\n",
       "      <td>[leaf]</td>\n",
       "      <td>→</td>\n",
       "      <td>[sheet, of, paper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>624</td>\n",
       "      <td>[to, see, to, look, at]</td>\n",
       "      <td>→</td>\n",
       "      <td>[to, have, an, appearance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                        word1 shift_dir                       word2\n",
       "0  5548                       [pope]         →                [ruff, fish]\n",
       "1  6550                        [hat]         →             [mushroom, cap]\n",
       "2   750  [to, search, to, look, for]         ↔                  [to, want]\n",
       "3  4864                      [heart]         →         [hearts, in, cards]\n",
       "4  6858                    [country]         →                    [turkey]\n",
       "5   509          [comb, of, a, bird]         —                      [comb]\n",
       "6   973              [to, stand, up]         →         [to, revolt, rebel]\n",
       "7  1303         [to, pull, to, draw]         →          [to, slow, linger]\n",
       "8   751                       [leaf]         →          [sheet, of, paper]\n",
       "9   624      [to, see, to, look, at]         →  [to, have, an, appearance]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############################## EVALUATION USING DATSEMSHIFT 3.0 #######################################\n",
    "\n",
    "\n",
    "# Open semShift.txt file\n",
    "with open(\"../semShift.txt\") as file:\n",
    "    # Split the lines of the file by tabs\n",
    "    lines = [line.rstrip().split('\\t') for line in file]\n",
    "\n",
    "# lines is the first four strings of each line\n",
    "lines = [line[:4] for line in lines]\n",
    "# print(lines[:3])\n",
    "\n",
    "# Load DatSemShift dataset in pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../semShift.csv', header=None)\n",
    "df.columns = ['id', 'word1', 'shift_dir', 'word2']\n",
    "df.head(10)\n",
    "\n",
    "# clean non-alphabet chars in the string in word1 and split the string\n",
    "df['word1'] = df['word1'].str.replace('[^a-zA-Z]', ' ').str.split()\n",
    "# clean non-alphabet chars in the string in word2 and split the string\n",
    "df['word2'] = df['word2'].str.replace('[^a-zA-Z]', ' ').str.split()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>word1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>word2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5548</td>\n",
       "      <td>[pope]</td>\n",
       "      <td>→</td>\n",
       "      <td>[ruff, fish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6550</td>\n",
       "      <td>[hat]</td>\n",
       "      <td>→</td>\n",
       "      <td>[mushroom, cap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750</td>\n",
       "      <td>[search, look]</td>\n",
       "      <td>↔</td>\n",
       "      <td>[want]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4864</td>\n",
       "      <td>[heart]</td>\n",
       "      <td>→</td>\n",
       "      <td>[hearts, cards]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6858</td>\n",
       "      <td>[country]</td>\n",
       "      <td>→</td>\n",
       "      <td>[turkey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>509</td>\n",
       "      <td>[comb, bird]</td>\n",
       "      <td>—</td>\n",
       "      <td>[comb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>973</td>\n",
       "      <td>[stand]</td>\n",
       "      <td>→</td>\n",
       "      <td>[revolt, rebel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1303</td>\n",
       "      <td>[pull, draw]</td>\n",
       "      <td>→</td>\n",
       "      <td>[slow, linger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>751</td>\n",
       "      <td>[leaf]</td>\n",
       "      <td>→</td>\n",
       "      <td>[sheet, paper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>624</td>\n",
       "      <td>[see, look]</td>\n",
       "      <td>→</td>\n",
       "      <td>[appearance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           word1 shift_dir            word2\n",
       "0  5548          [pope]         →     [ruff, fish]\n",
       "1  6550           [hat]         →  [mushroom, cap]\n",
       "2   750  [search, look]         ↔           [want]\n",
       "3  4864         [heart]         →  [hearts, cards]\n",
       "4  6858       [country]         →         [turkey]\n",
       "5   509    [comb, bird]         —           [comb]\n",
       "6   973         [stand]         →  [revolt, rebel]\n",
       "7  1303    [pull, draw]         →   [slow, linger]\n",
       "8   751          [leaf]         →   [sheet, paper]\n",
       "9   624     [see, look]         →     [appearance]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from word1\n",
    "df['word1'] = df['word1'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# Remove stop words from word2\n",
    "df['word2'] = df['word2'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30922a6c5955a1d7417cdb8440e3c5be4b48d1d766725ea5fb70086fab274955"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
