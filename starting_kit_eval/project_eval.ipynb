{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SemEval2020 Task 1 Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Train two models for two time points and align them using Orthogonal Procrustes\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Training word2vec model on english practice corpus 1\n",
    "model = Word2Vec(corpus_file=\"test_data_public/english/corpus1/lemma/ccoha1.txt\", vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "model.save(\"test_data_public/english/corpus1/lemma/ccoha1.model\")\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"test_data_public/english/corpus1/lemma/ccoha1.wv\")\n",
    "\n",
    "wv = KeyedVectors.load(\"test_data_public/english/corpus1/lemma/ccoha1.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train second word2vec model using corpus2 lines\n",
    "model2 = Word2Vec(corpus_file=\"test_data_public/english/corpus2/lemma/ccoha2.txt\", vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "model2.save(\"test_data_public/english/corpus2/lemma/ccoha2.model\")\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors2 = model2.wv\n",
    "word_vectors2.save(\"test_data_public/english/corpus2/lemma/ccoha2.wv\")\n",
    "\n",
    "wv2 = KeyedVectors.load(\"test_data_public/english/corpus2/lemma/ccoha2.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attack_nn', 'bag_nn', 'ball_nn', 'bit_nn', 'chairman_nn', 'circle_vb', 'contemplation_nn', 'donkey_nn', 'edge_nn', 'face_nn', 'fiction_nn', 'gas_nn', 'graft_nn', 'head_nn', 'land_nn', 'lane_nn', 'lass_nn', 'multitude_nn', 'ounce_nn', 'part_nn', 'pin_vb', 'plane_nn', 'player_nn', 'prop_nn', 'quilt_nn', 'rag_nn', 'record_nn', 'relationship_nn', 'risk_nn', 'savage_nn', 'stab_nn', 'stroke_vb', 'thump_nn', 'tip_vb', 'tree_nn', 'twist_nn', 'word_nn']\n"
     ]
    }
   ],
   "source": [
    "# Load target words from targets.txt\n",
    "target_words = []\n",
    "with open(\"test_data_public/english/targets.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        target_words.append(line.strip())\n",
    "\n",
    "print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align wv and wv2 using Orthogonal Procrustes\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# print(wv.get_normed_vectors(), wv2.get_normed_vectors())\n",
    "\n",
    "# Get vectors for target words\n",
    "wv_target_vectors = np.array([wv.get_vector(word) for word in target_words])\n",
    "wv2_target_vectors = np.array([wv2.get_vector(word) for word in target_words])\n",
    "\n",
    "wv_target_vectors_mu = wv_target_vectors.mean(axis=0)\n",
    "wv_target_vectors_centered = wv_target_vectors - wv_target_vectors_mu\n",
    "\n",
    "wv2_target_vectors_mu = wv2_target_vectors.mean(axis=0)\n",
    "wv2_target_vectors_centered = wv2_target_vectors - wv2_target_vectors_mu\n",
    "\n",
    "R, sca = orthogonal_procrustes(wv_target_vectors_centered, wv2_target_vectors_centered)\n",
    "# print(R, sca)\n",
    "scale = sca / np.square(norm(wv_target_vectors_centered))\n",
    "\n",
    "wv2_target_vectors_approx = scale * np.dot(wv_target_vectors_centered, R) + wv2_target_vectors_mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy (align):  0.5945945945945946\n"
     ]
    }
   ],
   "source": [
    "# Task 1 - Compute target words cosine similarity between aligned vectors and wv2 vectors\n",
    "task1_similarities = []\n",
    "for i, word in enumerate(target_words):\n",
    "    # print(word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1)))\n",
    "    task1_similarities.append([word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1))[0][0]])\n",
    "\n",
    "# print(task1_similarities)\n",
    "\n",
    "# Load truth file\n",
    "task1_truth = []\n",
    "with open(\"test_data_public/english/truth/binary.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        task1_truth.append(line.strip().split())\n",
    "\n",
    "# For each list in task1_similarities, check if value is less than 0.9, assign 1 if true, 0 if false\n",
    "task1_res = []\n",
    "for i, word in enumerate(task1_similarities):\n",
    "    if word[1] < 0.9:\n",
    "        task1_res.append([word[0], 1])\n",
    "    else:\n",
    "        task1_res.append([word[0], 0])\n",
    "\n",
    "# Compare task1_res and task1_truth\n",
    "task1_correct = 0\n",
    "for i, word in enumerate(task1_res):\n",
    "    if word[1] == int(task1_truth[i][1]):\n",
    "        task1_correct += 1\n",
    "\n",
    "print(\"Task 1 accuracy (align): \", task1_correct/len(task1_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Train a combined model on both corpora (target words in corpus2 are changed to target_word_)\n",
    "\n",
    "# Store corpus1 and corpus2 lines together\n",
    "c1_c2_lines = []\n",
    "\n",
    "with open(\"test_data_public/english/corpus1/lemma/ccoha1.txt\") as file:\n",
    "    c1_lines = [line.rstrip().split() for line in file]\n",
    "    c1_c2_lines.extend(c1_lines)\n",
    "\n",
    "with open(\"test_data_public/english/corpus2/lemma/ccoha2.txt\") as file:\n",
    "    c2_lines = [line.rstrip().split() for line in file]\n",
    "    for line in c2_lines:\n",
    "        for word in line:\n",
    "            if word in target_words:\n",
    "                line[line.index(word)] = word + '_'\n",
    "    c1_c2_lines.extend(c2_lines)\n",
    "\n",
    "# Train combined model\n",
    "model_combined = Word2Vec(c1_c2_lines, vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors_combined = model_combined.wv\n",
    "word_vectors_combined.save(\"test_data_public/english/ccoha_combined.wv\")\n",
    "\n",
    "wv_combined = KeyedVectors.load(\"test_data_public/english/ccoha_combined.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy (combined):  0.6756756756756757\n"
     ]
    }
   ],
   "source": [
    "# Task 1 - Compute target words cosine distance in combined word vector\n",
    "task1_similarities_combined = []\n",
    "for i, word in enumerate(target_words):\n",
    "    # print(word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1)))\n",
    "    task1_similarities_combined.append([word, cosine_similarity(wv_combined.get_vector(word).reshape(1,-1), wv_combined.get_vector(word + '_').reshape(1,-1))[0][0]])\n",
    "\n",
    "# print(task1_similarities_combined)\n",
    "\n",
    "# Compare task1_similarities_combined and task1_truth\n",
    "task1_combined_res = []\n",
    "for i, word in enumerate(task1_similarities_combined):\n",
    "    if word[1] < 0.65:\n",
    "        task1_combined_res.append([word[0], 1])\n",
    "    else:\n",
    "        task1_combined_res.append([word[0], 0])\n",
    "\n",
    "task1_combined_correct = 0\n",
    "for i, word in enumerate(task1_combined_res):\n",
    "    if word[1] == int(task1_truth[i][1]):\n",
    "        task1_combined_correct += 1\n",
    "\n",
    "print(\"Task 1 accuracy (combined): \", task1_combined_correct/len(task1_combined_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 spearman correlation (align):  SpearmanrResult(correlation=-0.06425988601233414, pvalue=0.7055400503518773)\n",
      "Task 2 spearman correlation (combined):  SpearmanrResult(correlation=-0.2616634103860174, pvalue=0.11772270339847497)\n"
     ]
    }
   ],
   "source": [
    "#  Task 2 - Compute spearman correlation between target words cosine similarity in wv2 and wv2_aligned\\n\",\n",
    "from scipy import stats\n",
    "# Load task2 truth file\n",
    "task2_truth = []\n",
    "# open truth graded file\n",
    "\n",
    "with open(\"test_data_public/english/truth/graded.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        task2_truth.append(line.strip().split()[1])\n",
    "\n",
    "# task2_ranking = the second item in for every item in task1_similarities\n",
    "task2_ranking_align = [item[1] for item in task1_similarities]\n",
    "\n",
    "# task2_ranking_combined = the second item in for every item in task1_similarities_combined\n",
    "task2_ranking_combined = [item[1] for item in task1_similarities_combined]\n",
    "\n",
    "# Compute spearman correlation\n",
    "task2_align_corr = stats.spearmanr(task2_ranking_align, task2_truth)\n",
    "\n",
    "task2_combined_corr = stats.spearmanr(task2_ranking_combined, task2_truth)\n",
    "\n",
    "print(\"Task 2 spearman correlation (align): \", task2_align_corr)\n",
    "print(\"Task 2 spearman correlation (combined): \", task2_combined_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5548</td>\n",
       "      <td>[pope]</td>\n",
       "      <td>→</td>\n",
       "      <td>[ruff, fish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6550</td>\n",
       "      <td>[hat]</td>\n",
       "      <td>→</td>\n",
       "      <td>[mushroom, cap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750</td>\n",
       "      <td>[to, search, to, look, for]</td>\n",
       "      <td>↔</td>\n",
       "      <td>[to, want]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4864</td>\n",
       "      <td>[heart]</td>\n",
       "      <td>→</td>\n",
       "      <td>[hearts, in, cards]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6858</td>\n",
       "      <td>[country]</td>\n",
       "      <td>→</td>\n",
       "      <td>[turkey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>509</td>\n",
       "      <td>[comb, of, a, bird]</td>\n",
       "      <td>—</td>\n",
       "      <td>[comb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>973</td>\n",
       "      <td>[to, stand, up]</td>\n",
       "      <td>→</td>\n",
       "      <td>[to, revolt, rebel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1303</td>\n",
       "      <td>[to, pull, to, draw]</td>\n",
       "      <td>→</td>\n",
       "      <td>[to, slow, linger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>751</td>\n",
       "      <td>[leaf]</td>\n",
       "      <td>→</td>\n",
       "      <td>[sheet, of, paper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>624</td>\n",
       "      <td>[to, see, to, look, at]</td>\n",
       "      <td>→</td>\n",
       "      <td>[to, have, an, appearance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                     meaning1 shift_dir                    meaning2\n",
       "0  5548                       [pope]         →                [ruff, fish]\n",
       "1  6550                        [hat]         →             [mushroom, cap]\n",
       "2   750  [to, search, to, look, for]         ↔                  [to, want]\n",
       "3  4864                      [heart]         →         [hearts, in, cards]\n",
       "4  6858                    [country]         →                    [turkey]\n",
       "5   509          [comb, of, a, bird]         —                      [comb]\n",
       "6   973              [to, stand, up]         →         [to, revolt, rebel]\n",
       "7  1303         [to, pull, to, draw]         →          [to, slow, linger]\n",
       "8   751                       [leaf]         →          [sheet, of, paper]\n",
       "9   624      [to, see, to, look, at]         →  [to, have, an, appearance]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################## EVALUATION USING DATSEMSHIFT 3.0 #######################################\n",
    "\n",
    "\n",
    "# Open semShift.txt file\n",
    "with open(\"../semShift.txt\") as file:\n",
    "    # Split the lines of the file by tabs\n",
    "    lines = [line.rstrip().split('\\t') for line in file]\n",
    "\n",
    "# lines is the first four strings of each line\n",
    "lines = [line[:4] for line in lines]\n",
    "# print(lines[:3])\n",
    "\n",
    "# Load DatSemShift dataset in pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../semShift.csv', header=None)\n",
    "df.columns = ['id', 'meaning1', 'shift_dir', 'meaning2']\n",
    "df.head(10)\n",
    "\n",
    "# clean non-alphabet chars in the string in meaning1 and split the string\n",
    "df['meaning1'] = df['meaning1'].str.replace('[^a-zA-Z]', ' ').str.split()\n",
    "# clean non-alphabet chars in the string in meaning2 and split the string\n",
    "df['meaning2'] = df['meaning2'].str.replace('[^a-zA-Z]', ' ').str.split()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5548</td>\n",
       "      <td>[pope]</td>\n",
       "      <td>→</td>\n",
       "      <td>[ruff, fish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6550</td>\n",
       "      <td>[hat]</td>\n",
       "      <td>→</td>\n",
       "      <td>[mushroom, cap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750</td>\n",
       "      <td>[search, look]</td>\n",
       "      <td>↔</td>\n",
       "      <td>[want]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4864</td>\n",
       "      <td>[heart]</td>\n",
       "      <td>→</td>\n",
       "      <td>[hearts, cards]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6858</td>\n",
       "      <td>[country]</td>\n",
       "      <td>→</td>\n",
       "      <td>[turkey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>509</td>\n",
       "      <td>[comb, bird]</td>\n",
       "      <td>—</td>\n",
       "      <td>[comb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>973</td>\n",
       "      <td>[stand]</td>\n",
       "      <td>→</td>\n",
       "      <td>[revolt, rebel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1303</td>\n",
       "      <td>[pull, draw]</td>\n",
       "      <td>→</td>\n",
       "      <td>[slow, linger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>751</td>\n",
       "      <td>[leaf]</td>\n",
       "      <td>→</td>\n",
       "      <td>[sheet, paper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>624</td>\n",
       "      <td>[see, look]</td>\n",
       "      <td>→</td>\n",
       "      <td>[appearance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1148</td>\n",
       "      <td>[cool, intr]</td>\n",
       "      <td>→</td>\n",
       "      <td>[calm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1476</td>\n",
       "      <td>[remain, stay]</td>\n",
       "      <td>→</td>\n",
       "      <td>[corpse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1776</td>\n",
       "      <td>[female, calf]</td>\n",
       "      <td>→</td>\n",
       "      <td>[girl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2295</td>\n",
       "      <td>[buttock]</td>\n",
       "      <td>—</td>\n",
       "      <td>[edge, border]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2540</td>\n",
       "      <td>[common, shared, several, people]</td>\n",
       "      <td>→</td>\n",
       "      <td>[vulgar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>601</td>\n",
       "      <td>[wood, timber]</td>\n",
       "      <td>—</td>\n",
       "      <td>[forest]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>242</td>\n",
       "      <td>[grasp, seize]</td>\n",
       "      <td>→</td>\n",
       "      <td>[understand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2978</td>\n",
       "      <td>[poppy, Papaver, somniferum]</td>\n",
       "      <td>—</td>\n",
       "      <td>[red]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>471</td>\n",
       "      <td>[eye]</td>\n",
       "      <td>→</td>\n",
       "      <td>[look, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5617</td>\n",
       "      <td>[key, door]</td>\n",
       "      <td>→</td>\n",
       "      <td>[keystone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1042</td>\n",
       "      <td>[drive, force, move]</td>\n",
       "      <td>→</td>\n",
       "      <td>[drive, car]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>337</td>\n",
       "      <td>[strike, hit]</td>\n",
       "      <td>→</td>\n",
       "      <td>[astonish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1311</td>\n",
       "      <td>[magpie, Pica, pica]</td>\n",
       "      <td>→</td>\n",
       "      <td>[talkative, person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2033</td>\n",
       "      <td>[fine, powder, like]</td>\n",
       "      <td>—</td>\n",
       "      <td>[thin, object]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1138</td>\n",
       "      <td>[bridle]</td>\n",
       "      <td>→</td>\n",
       "      <td>[subjugate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5057</td>\n",
       "      <td>[dry]</td>\n",
       "      <td>→</td>\n",
       "      <td>[dry, wine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6755</td>\n",
       "      <td>[rupture, hernia]</td>\n",
       "      <td>→</td>\n",
       "      <td>[burstwort, herniaria]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2860</td>\n",
       "      <td>[wide, broad]</td>\n",
       "      <td>—</td>\n",
       "      <td>[far, away]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3764</td>\n",
       "      <td>[straight]</td>\n",
       "      <td>—</td>\n",
       "      <td>[heterosexual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4763</td>\n",
       "      <td>[foreigner]</td>\n",
       "      <td>→</td>\n",
       "      <td>[thorny, plant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4825</td>\n",
       "      <td>[bed]</td>\n",
       "      <td>→</td>\n",
       "      <td>[garden, bed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>450</td>\n",
       "      <td>[eye]</td>\n",
       "      <td>→</td>\n",
       "      <td>[opening, hole]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1836</td>\n",
       "      <td>[friend]</td>\n",
       "      <td>→</td>\n",
       "      <td>[beloved]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1395</td>\n",
       "      <td>[cross, n]</td>\n",
       "      <td>→</td>\n",
       "      <td>[crossbill, Loxia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5479</td>\n",
       "      <td>[drink]</td>\n",
       "      <td>→</td>\n",
       "      <td>[consume, alcoholic, beverages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3191</td>\n",
       "      <td>[sky]</td>\n",
       "      <td>—</td>\n",
       "      <td>[cloud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1258</td>\n",
       "      <td>[drop, liquid]</td>\n",
       "      <td>—</td>\n",
       "      <td>[die]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4558</td>\n",
       "      <td>[Ranunculus]</td>\n",
       "      <td>—</td>\n",
       "      <td>[cornflower, Centaurea]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6019</td>\n",
       "      <td>[rain]</td>\n",
       "      <td>→</td>\n",
       "      <td>[earthworm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4022</td>\n",
       "      <td>[father]</td>\n",
       "      <td>—</td>\n",
       "      <td>[defender]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1404</td>\n",
       "      <td>[pregnant]</td>\n",
       "      <td>→</td>\n",
       "      <td>[fraught, consequences]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6007</td>\n",
       "      <td>[beard]</td>\n",
       "      <td>→</td>\n",
       "      <td>[awn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7149</td>\n",
       "      <td>[beetle]</td>\n",
       "      <td>→</td>\n",
       "      <td>[enthusiastic, person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1645</td>\n",
       "      <td>[tail]</td>\n",
       "      <td>—</td>\n",
       "      <td>[rudder, boat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6207</td>\n",
       "      <td>[noise]</td>\n",
       "      <td>→</td>\n",
       "      <td>[Bitis, arietans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5529</td>\n",
       "      <td>[field]</td>\n",
       "      <td>→</td>\n",
       "      <td>[fieldfare]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>7196</td>\n",
       "      <td>[gold]</td>\n",
       "      <td>→</td>\n",
       "      <td>[goldenrod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4179</td>\n",
       "      <td>[strike, hit]</td>\n",
       "      <td>↔</td>\n",
       "      <td>[strike, lightning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3676</td>\n",
       "      <td>[transparent]</td>\n",
       "      <td>→</td>\n",
       "      <td>[comprehensible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6380</td>\n",
       "      <td>[open, eyed]</td>\n",
       "      <td>→</td>\n",
       "      <td>[vigilant]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                           meaning1 shift_dir  \\\n",
       "0   5548                             [pope]         →   \n",
       "1   6550                              [hat]         →   \n",
       "2    750                     [search, look]         ↔   \n",
       "3   4864                            [heart]         →   \n",
       "4   6858                          [country]         →   \n",
       "5    509                       [comb, bird]         —   \n",
       "6    973                            [stand]         →   \n",
       "7   1303                       [pull, draw]         →   \n",
       "8    751                             [leaf]         →   \n",
       "9    624                        [see, look]         →   \n",
       "10  1148                       [cool, intr]         →   \n",
       "11  1476                     [remain, stay]         →   \n",
       "12  1776                     [female, calf]         →   \n",
       "13  2295                          [buttock]         —   \n",
       "14  2540  [common, shared, several, people]         →   \n",
       "15   601                     [wood, timber]         —   \n",
       "16   242                     [grasp, seize]         →   \n",
       "17  2978       [poppy, Papaver, somniferum]         —   \n",
       "18   471                              [eye]         →   \n",
       "19  5617                        [key, door]         →   \n",
       "20  1042               [drive, force, move]         →   \n",
       "21   337                      [strike, hit]         →   \n",
       "22  1311               [magpie, Pica, pica]         →   \n",
       "23  2033               [fine, powder, like]         —   \n",
       "24  1138                           [bridle]         →   \n",
       "25  5057                              [dry]         →   \n",
       "26  6755                  [rupture, hernia]         →   \n",
       "27  2860                      [wide, broad]         —   \n",
       "28  3764                         [straight]         —   \n",
       "29  4763                        [foreigner]         →   \n",
       "30  4825                              [bed]         →   \n",
       "31   450                              [eye]         →   \n",
       "32  1836                           [friend]         →   \n",
       "33  1395                         [cross, n]         →   \n",
       "34  5479                            [drink]         →   \n",
       "35  3191                              [sky]         —   \n",
       "36  1258                     [drop, liquid]         —   \n",
       "37  4558                       [Ranunculus]         —   \n",
       "38  6019                             [rain]         →   \n",
       "39  4022                           [father]         —   \n",
       "40  1404                         [pregnant]         →   \n",
       "41  6007                            [beard]         →   \n",
       "42  7149                           [beetle]         →   \n",
       "43  1645                             [tail]         —   \n",
       "44  6207                            [noise]         →   \n",
       "45  5529                            [field]         →   \n",
       "46  7196                             [gold]         →   \n",
       "47  4179                      [strike, hit]         ↔   \n",
       "48  3676                      [transparent]         →   \n",
       "49  6380                       [open, eyed]         →   \n",
       "\n",
       "                           meaning2  \n",
       "0                      [ruff, fish]  \n",
       "1                   [mushroom, cap]  \n",
       "2                            [want]  \n",
       "3                   [hearts, cards]  \n",
       "4                          [turkey]  \n",
       "5                            [comb]  \n",
       "6                   [revolt, rebel]  \n",
       "7                    [slow, linger]  \n",
       "8                    [sheet, paper]  \n",
       "9                      [appearance]  \n",
       "10                           [calm]  \n",
       "11                         [corpse]  \n",
       "12                           [girl]  \n",
       "13                   [edge, border]  \n",
       "14                         [vulgar]  \n",
       "15                         [forest]  \n",
       "16                     [understand]  \n",
       "17                            [red]  \n",
       "18                        [look, n]  \n",
       "19                       [keystone]  \n",
       "20                     [drive, car]  \n",
       "21                       [astonish]  \n",
       "22              [talkative, person]  \n",
       "23                   [thin, object]  \n",
       "24                      [subjugate]  \n",
       "25                      [dry, wine]  \n",
       "26           [burstwort, herniaria]  \n",
       "27                      [far, away]  \n",
       "28                   [heterosexual]  \n",
       "29                  [thorny, plant]  \n",
       "30                    [garden, bed]  \n",
       "31                  [opening, hole]  \n",
       "32                        [beloved]  \n",
       "33               [crossbill, Loxia]  \n",
       "34  [consume, alcoholic, beverages]  \n",
       "35                          [cloud]  \n",
       "36                            [die]  \n",
       "37          [cornflower, Centaurea]  \n",
       "38                      [earthworm]  \n",
       "39                       [defender]  \n",
       "40          [fraught, consequences]  \n",
       "41                            [awn]  \n",
       "42           [enthusiastic, person]  \n",
       "43                   [rudder, boat]  \n",
       "44                [Bitis, arietans]  \n",
       "45                      [fieldfare]  \n",
       "46                      [goldenrod]  \n",
       "47              [strike, lightning]  \n",
       "48                 [comprehensible]  \n",
       "49                       [vigilant]  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from word1\n",
    "df['meaning1'] = df['meaning1'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# Remove stop words from word2\n",
    "df['meaning2'] = df['meaning2'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shifts in df_shift_res:  680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope</td>\n",
       "      <td>→</td>\n",
       "      <td>fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hat</td>\n",
       "      <td>→</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>look</td>\n",
       "      <td>↔</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>country</td>\n",
       "      <td>→</td>\n",
       "      <td>turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>—</td>\n",
       "      <td>comb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stand</td>\n",
       "      <td>→</td>\n",
       "      <td>rebel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>draw</td>\n",
       "      <td>→</td>\n",
       "      <td>linger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>leaf</td>\n",
       "      <td>→</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>look</td>\n",
       "      <td>→</td>\n",
       "      <td>appearance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cool</td>\n",
       "      <td>→</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  meaning1 shift_dir    meaning2\n",
       "0     pope         →        fish\n",
       "1      hat         →         cap\n",
       "2     look         ↔        want\n",
       "3  country         →      turkey\n",
       "4     bird         —        comb\n",
       "5    stand         →       rebel\n",
       "6     draw         →      linger\n",
       "7     leaf         →       paper\n",
       "8     look         →  appearance\n",
       "9     cool         →        calm"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation Approach 1: use the first word in meaning1 and meaning2 that are in the vocabulary\n",
    "# Align the set of traget word vectors\n",
    "# Compute cosine similarity between the two target words in c1 and c2\n",
    "# if cosine similarity changed in the direction of shift direction, then the prediction is correct\n",
    "\n",
    "# make an empty data frame to store the results\n",
    "df_shift_res = pd.DataFrame(columns=['meaning1', 'shift_dir', 'meaning2'])\n",
    "num_shifts = 0\n",
    "\n",
    "# for each row in df\n",
    "for index, row in df.iterrows():\n",
    "    meaning1_word = \"\"\n",
    "    meaning2_word = \"\"\n",
    "    # for each word in meaning1\n",
    "    for word in row['meaning1']:\n",
    "        # if word is in the vocabulary\n",
    "        if word in wv and word in wv2:\n",
    "            meaning1_word = word\n",
    "    # for each word in meaning2\n",
    "    for word in row['meaning2']:\n",
    "        # if word is in the vocabulary\n",
    "        if word in wv and word in wv2:\n",
    "            meaning2_word = word\n",
    "\n",
    "    # if meaning1_word and meaning2_word are not empty\n",
    "    if meaning1_word and meaning2_word:\n",
    "        # if meaning1_word and meaning2_word are not the same word\n",
    "        if meaning1_word != meaning2_word:\n",
    "            num_shifts += 1\n",
    "            # add meaning1_word and meaning2_word and their cosine similarity to shifts_list\n",
    "            df_shift_res = df_shift_res.append({'meaning1': meaning1_word, 'shift_dir': row['shift_dir'], 'meaning2': meaning2_word}, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"Number of shifts in df_shift_res: \", len(df_shift_res))\n",
    "df_shift_res.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1.1: Separate embedding no alignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24862945, -0.3373772, -1.0], [0.8995824, 0.81243926, -1.0], [0.1387261, 0.21904372, 1.0], [0.11960769, 0.36179477, 1.0], [0.41633216, 0.28525874, -1.0], [0.17037529, 0.15207587, -1.0], [0.1645097, 0.3066887, 1.0], [0.21008238, 0.34180748, 1.0], [0.20084272, 0.25881878, 1.0], [0.5883417, 0.62778693, 1.0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>cos_sim_c1</th>\n",
       "      <th>cos_sim_c2</th>\n",
       "      <th>detected_dir_1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope</td>\n",
       "      <td>→</td>\n",
       "      <td>fish</td>\n",
       "      <td>-0.248629</td>\n",
       "      <td>-0.337377</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hat</td>\n",
       "      <td>→</td>\n",
       "      <td>cap</td>\n",
       "      <td>0.899582</td>\n",
       "      <td>0.812439</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>look</td>\n",
       "      <td>↔</td>\n",
       "      <td>want</td>\n",
       "      <td>0.138726</td>\n",
       "      <td>0.219044</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>country</td>\n",
       "      <td>→</td>\n",
       "      <td>turkey</td>\n",
       "      <td>0.119608</td>\n",
       "      <td>0.361795</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>—</td>\n",
       "      <td>comb</td>\n",
       "      <td>0.416332</td>\n",
       "      <td>0.285259</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stand</td>\n",
       "      <td>→</td>\n",
       "      <td>rebel</td>\n",
       "      <td>0.170375</td>\n",
       "      <td>0.152076</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>draw</td>\n",
       "      <td>→</td>\n",
       "      <td>linger</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>0.306689</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>leaf</td>\n",
       "      <td>→</td>\n",
       "      <td>paper</td>\n",
       "      <td>0.210082</td>\n",
       "      <td>0.341807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>look</td>\n",
       "      <td>→</td>\n",
       "      <td>appearance</td>\n",
       "      <td>0.200843</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cool</td>\n",
       "      <td>→</td>\n",
       "      <td>calm</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  meaning1 shift_dir    meaning2  cos_sim_c1  cos_sim_c2  detected_dir_1.1\n",
       "0     pope         →        fish   -0.248629   -0.337377              -1.0\n",
       "1      hat         →         cap    0.899582    0.812439              -1.0\n",
       "2     look         ↔        want    0.138726    0.219044               1.0\n",
       "3  country         →      turkey    0.119608    0.361795               1.0\n",
       "4     bird         —        comb    0.416332    0.285259              -1.0\n",
       "5    stand         →       rebel    0.170375    0.152076              -1.0\n",
       "6     draw         →      linger    0.164510    0.306689               1.0\n",
       "7     leaf         →       paper    0.210082    0.341807               1.0\n",
       "8     look         →  appearance    0.200843    0.258819               1.0\n",
       "9     cool         →        calm    0.588342    0.627787               1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute cosine similarity between meaning1 and meaning2 in df_shift_res\n",
    "detection_res_single_word = [] \n",
    "for i in range(len(df_shift_res)):\n",
    "    cos_sim_c1 = cosine_similarity(wv.get_vector(df_shift_res.iloc[i]['meaning1']).reshape(1, -1), wv.get_vector(df_shift_res.iloc[i]['meaning2']).reshape(1, -1))[0][0]\n",
    "    cos_sim_c2 = cosine_similarity(wv2.get_vector(df_shift_res.iloc[i]['meaning1']).reshape(1, -1), wv2.get_vector(df_shift_res.iloc[i]['meaning2']).reshape(1, -1))[0][0]\n",
    "    detection_res_single_word.append([cos_sim_c1, cos_sim_c2, np.sign(cos_sim_c2 - cos_sim_c1)])\n",
    "\n",
    "print(detection_res_single_word[:10])\n",
    "\n",
    "# add the cosine similarity of meaning1 and meaning2 in df_shift_res to df_shift_res\n",
    "df_shift_res['cos_sim_c1'] = [detection_res_single_word[i][0] for i in range(len(detection_res_single_word))]\n",
    "df_shift_res['cos_sim_c2'] = [detection_res_single_word[i][1] for i in range(len(detection_res_single_word))]\n",
    "df_shift_res['detected_dir_1.1'] = [detection_res_single_word[i][2] for i in range(len(detection_res_single_word))]\n",
    "\n",
    "df_shift_res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly predicted shifts:  404\n",
      "Number of total shifts:  680\n",
      "Accuracy:  0.5941176470588235\n"
     ]
    }
   ],
   "source": [
    "# count # of correctly predicted shifts\n",
    "correct_shifts = 0\n",
    "for i in range(len(df_shift_res)):\n",
    "    if df_shift_res.iloc[i]['detected_dir_1.1'] == 1.0:\n",
    "        correct_shifts += 1\n",
    "\n",
    "print(\"Number of correctly predicted shifts: \", correct_shifts)\n",
    "print(\"Number of total shifts: \", len(df_shift_res))\n",
    "print(\"Accuracy: \", correct_shifts / len(df_shift_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1.2: Separate embedding with alignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import orthogonal_procrustes\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# join all meaning1 and meaning2 words in one list\n",
    "sem_shift_words = df_shift_res['meaning1'].tolist()\n",
    "meaning2_words = df_shift_res['meaning2'].tolist()\n",
    "sem_shift_words.extend(meaning2_words)\n",
    "\n",
    "# get the vectors of the words in sem_shift_words in wv and wv2 to do orthogonal procrustes alignment\n",
    "wv_sem_shift_vecs = np.array([wv.get_vector(word) for word in sem_shift_words])\n",
    "wv2_sem_shift_vecs = np.array([wv2.get_vector(word) for word in sem_shift_words])\n",
    "\n",
    "wv_sem_shift_vecs_mu = np.mean(wv_sem_shift_vecs, axis=0)\n",
    "wv_sem_shift_vecs_centered = wv_sem_shift_vecs - wv_sem_shift_vecs_mu\n",
    "\n",
    "wv2_sem_shift_vecs_mu = np.mean(wv2_sem_shift_vecs, axis=0)\n",
    "wv2_sem_shift_vecs_centered = wv2_sem_shift_vecs - wv2_sem_shift_vecs_mu\n",
    "\n",
    "R, sca = orthogonal_procrustes(wv_sem_shift_vecs_centered, wv2_sem_shift_vecs_centered)\n",
    "scale = sca / np.square(norm(wv_sem_shift_vecs_centered))\n",
    "\n",
    "wv2_sem_shift_approx = np.dot(wv_sem_shift_vecs_centered, R) * scale + wv2_sem_shift_vecs_mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15776566, 0.045846466, -1.0], [0.5554598, 0.32201913, -1.0], [0.30379307, 0.18748595, -1.0], [0.23062642, 0.14869829, -1.0], [0.56373936, 0.5832027, 1.0], [0.22429632, 0.27559346, 1.0], [0.18340445, 0.05448911, -1.0], [0.4448002, 0.42315954, -1.0], [0.04751588, 0.04989227, 1.0], [0.25186875, 0.09505028, -1.0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>cos_sim_c1</th>\n",
       "      <th>cos_sim_c2</th>\n",
       "      <th>detected_dir_1.1</th>\n",
       "      <th>cos_sim_c1_aligned</th>\n",
       "      <th>cos_sim_c2_aligned</th>\n",
       "      <th>detected_dir_1.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope</td>\n",
       "      <td>→</td>\n",
       "      <td>fish</td>\n",
       "      <td>-0.248629</td>\n",
       "      <td>-0.337377</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.157766</td>\n",
       "      <td>0.045846</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hat</td>\n",
       "      <td>→</td>\n",
       "      <td>cap</td>\n",
       "      <td>0.899582</td>\n",
       "      <td>0.812439</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.555460</td>\n",
       "      <td>0.322019</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>look</td>\n",
       "      <td>↔</td>\n",
       "      <td>want</td>\n",
       "      <td>0.138726</td>\n",
       "      <td>0.219044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.303793</td>\n",
       "      <td>0.187486</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>country</td>\n",
       "      <td>→</td>\n",
       "      <td>turkey</td>\n",
       "      <td>0.119608</td>\n",
       "      <td>0.361795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230626</td>\n",
       "      <td>0.148698</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>—</td>\n",
       "      <td>comb</td>\n",
       "      <td>0.416332</td>\n",
       "      <td>0.285259</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.563739</td>\n",
       "      <td>0.583203</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stand</td>\n",
       "      <td>→</td>\n",
       "      <td>rebel</td>\n",
       "      <td>0.170375</td>\n",
       "      <td>0.152076</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.224296</td>\n",
       "      <td>0.275593</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>draw</td>\n",
       "      <td>→</td>\n",
       "      <td>linger</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>0.306689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.183404</td>\n",
       "      <td>0.054489</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>leaf</td>\n",
       "      <td>→</td>\n",
       "      <td>paper</td>\n",
       "      <td>0.210082</td>\n",
       "      <td>0.341807</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.423160</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>look</td>\n",
       "      <td>→</td>\n",
       "      <td>appearance</td>\n",
       "      <td>0.200843</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.047516</td>\n",
       "      <td>0.049892</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cool</td>\n",
       "      <td>→</td>\n",
       "      <td>calm</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.251869</td>\n",
       "      <td>0.095050</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  meaning1 shift_dir    meaning2  cos_sim_c1  cos_sim_c2  detected_dir_1.1  \\\n",
       "0     pope         →        fish   -0.248629   -0.337377              -1.0   \n",
       "1      hat         →         cap    0.899582    0.812439              -1.0   \n",
       "2     look         ↔        want    0.138726    0.219044               1.0   \n",
       "3  country         →      turkey    0.119608    0.361795               1.0   \n",
       "4     bird         —        comb    0.416332    0.285259              -1.0   \n",
       "5    stand         →       rebel    0.170375    0.152076              -1.0   \n",
       "6     draw         →      linger    0.164510    0.306689               1.0   \n",
       "7     leaf         →       paper    0.210082    0.341807               1.0   \n",
       "8     look         →  appearance    0.200843    0.258819               1.0   \n",
       "9     cool         →        calm    0.588342    0.627787               1.0   \n",
       "\n",
       "   cos_sim_c1_aligned  cos_sim_c2_aligned  detected_dir_1.2  \n",
       "0            0.157766            0.045846              -1.0  \n",
       "1            0.555460            0.322019              -1.0  \n",
       "2            0.303793            0.187486              -1.0  \n",
       "3            0.230626            0.148698              -1.0  \n",
       "4            0.563739            0.583203               1.0  \n",
       "5            0.224296            0.275593               1.0  \n",
       "6            0.183404            0.054489              -1.0  \n",
       "7            0.444800            0.423160              -1.0  \n",
       "8            0.047516            0.049892               1.0  \n",
       "9            0.251869            0.095050              -1.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_res_single_word_aligned = [] \n",
    "\n",
    "for i in range(len(df_shift_res)):\n",
    "    cos_sim_c1 = cosine_similarity(wv2_sem_shift_approx[i].reshape(1,-1), wv2_sem_shift_approx[i+len(df_shift_res)-1].reshape(1,-1))[0][0]\n",
    "    cos_sim_c2 = cosine_similarity(wv2_sem_shift_vecs[i].reshape(1, -1), wv2_sem_shift_vecs[i+len(df_shift_res)-1].reshape(1, -1))[0][0]\n",
    "    detection_res_single_word_aligned.append([cos_sim_c1, cos_sim_c2, np.sign(cos_sim_c2 - cos_sim_c1)])\n",
    "\n",
    "# add the cosine similarity of meaning1 and meaning2 in df_shift_res to df_shift_res\n",
    "df_shift_res['cos_sim_c1_aligned'] = [detection_res_single_word_aligned[i][0] for i in range(len(detection_res_single_word_aligned))]\n",
    "df_shift_res['cos_sim_c2_aligned'] = [detection_res_single_word_aligned[i][1] for i in range(len(detection_res_single_word_aligned))]\n",
    "df_shift_res['detected_dir_1.2'] = [detection_res_single_word_aligned[i][2] for i in range(len(detection_res_single_word_aligned))]\n",
    "\n",
    "df_shift_res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly predicted shifts:  193\n",
      "Number of total shifts:  680\n",
      "Accuracy:  0.2838235294117647\n"
     ]
    }
   ],
   "source": [
    "# count # of correctly predicted shifts\n",
    "correct_shifts = 0\n",
    "for i in range(len(df_shift_res)):\n",
    "    if df_shift_res.iloc[i]['detected_dir_1.2'] == 1.0:\n",
    "        correct_shifts += 1\n",
    "\n",
    "print(\"Number of correctly predicted shifts: \", correct_shifts)\n",
    "print(\"Number of total shifts: \", len(df_shift_res))\n",
    "print(\"Accuracy: \", correct_shifts / len(df_shift_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1.3: Train combined Embedding on both Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store corpus1 and corpus2 lines together\n",
    "c1_c2_lines = []\n",
    "\n",
    "with open(\"test_data_public/english/corpus1/lemma/ccoha1.txt\") as file:\n",
    "    c1_lines = [line.rstrip().split() for line in file]\n",
    "    c1_c2_lines.extend(c1_lines)\n",
    "\n",
    "with open(\"test_data_public/english/corpus2/lemma/ccoha2.txt\") as file:\n",
    "    c2_lines = [line.rstrip().split() for line in file]\n",
    "    for line in c2_lines:\n",
    "        for word in line:\n",
    "            if word in sem_shift_words:\n",
    "                line[line.index(word)] = word + '_'\n",
    "    c1_c2_lines.extend(c2_lines)\n",
    "\n",
    "# Train combined model\n",
    "model_combined = Word2Vec(c1_c2_lines, vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors_combined_sem_shift = model_combined.wv\n",
    "word_vectors_combined_sem_shift.save(\"test_data_public/english/ccoha_combined_sem_shift.wv\")\n",
    "\n",
    "wv_combined_sem_shift = KeyedVectors.load(\"test_data_public/english/ccoha_combined_sem_shift.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly predicted shifts:  327\n",
      "Number of total shifts:  680\n",
      "Accuracy:  0.4808823529411765\n"
     ]
    }
   ],
   "source": [
    "detection_res_single_word_combined = [] \n",
    "\n",
    "for i in range(len(df_shift_res)):\n",
    "    cos_sim_c1 = cosine_similarity(wv_combined_sem_shift.get_vector(df_shift_res.iloc[i]['meaning1']).reshape(1,-1), wv_combined_sem_shift.get_vector(df_shift_res.iloc[i]['meaning2']).reshape(1,-1))[0][0]\n",
    "    cos_sim_c2 = cosine_similarity(wv_combined_sem_shift.get_vector(df_shift_res.iloc[i]['meaning1'] + '_').reshape(1, -1), wv_combined_sem_shift.get_vector(df_shift_res.iloc[i]['meaning2'] + '_').reshape(1, -1))[0][0]\n",
    "    detection_res_single_word_combined.append([cos_sim_c1, cos_sim_c2, np.sign(cos_sim_c2 - cos_sim_c1)])\n",
    "\n",
    "# add the cosine similarity of meaning1 and meaning2 in df_shift_res to df_shift_res\n",
    "df_shift_res['cos_sim_c1_combined'] = [detection_res_single_word_combined[i][0] for i in range(len(detection_res_single_word_combined))]\n",
    "df_shift_res['cos_sim_c2_combined'] = [detection_res_single_word_combined[i][1] for i in range(len(detection_res_single_word_combined))]\n",
    "df_shift_res['detected_dir_1.3'] = [detection_res_single_word_combined[i][2] for i in range(len(detection_res_single_word_combined))]\n",
    "\n",
    "df_shift_res.head(10)\n",
    "\n",
    "# count # of correctly predicted shifts\n",
    "correct_shifts = 0\n",
    "for i in range(len(df_shift_res)):\n",
    "    if df_shift_res.iloc[i]['detected_dir_1.3'] == 1.0:\n",
    "        correct_shifts += 1\n",
    "\n",
    "print(\"Number of correctly predicted shifts: \", correct_shifts)\n",
    "print(\"Number of total shifts: \", len(df_shift_res))\n",
    "print(\"Accuracy: \", correct_shifts / len(df_shift_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>cos_sim_c1</th>\n",
       "      <th>cos_sim_c2</th>\n",
       "      <th>detected_dir_1.1</th>\n",
       "      <th>cos_sim_c1_aligned</th>\n",
       "      <th>cos_sim_c2_aligned</th>\n",
       "      <th>detected_dir_1.2</th>\n",
       "      <th>cos_sim_c1_combined</th>\n",
       "      <th>cos_sim_c2_combined</th>\n",
       "      <th>detected_dir_1.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope</td>\n",
       "      <td>→</td>\n",
       "      <td>fish</td>\n",
       "      <td>-0.248629</td>\n",
       "      <td>-0.337377</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.157766</td>\n",
       "      <td>0.045846</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.243716</td>\n",
       "      <td>-0.383739</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hat</td>\n",
       "      <td>→</td>\n",
       "      <td>cap</td>\n",
       "      <td>0.899582</td>\n",
       "      <td>0.812439</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.555460</td>\n",
       "      <td>0.322019</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.903475</td>\n",
       "      <td>0.792746</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>look</td>\n",
       "      <td>↔</td>\n",
       "      <td>want</td>\n",
       "      <td>0.138726</td>\n",
       "      <td>0.219044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.303793</td>\n",
       "      <td>0.187486</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.137415</td>\n",
       "      <td>0.234502</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>country</td>\n",
       "      <td>→</td>\n",
       "      <td>turkey</td>\n",
       "      <td>0.119608</td>\n",
       "      <td>0.361795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230626</td>\n",
       "      <td>0.148698</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.059941</td>\n",
       "      <td>0.419437</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>—</td>\n",
       "      <td>comb</td>\n",
       "      <td>0.416332</td>\n",
       "      <td>0.285259</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.563739</td>\n",
       "      <td>0.583203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.383995</td>\n",
       "      <td>0.333703</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stand</td>\n",
       "      <td>→</td>\n",
       "      <td>rebel</td>\n",
       "      <td>0.170375</td>\n",
       "      <td>0.152076</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.224296</td>\n",
       "      <td>0.275593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249531</td>\n",
       "      <td>0.217194</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>draw</td>\n",
       "      <td>→</td>\n",
       "      <td>linger</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>0.306689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.183404</td>\n",
       "      <td>0.054489</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.218070</td>\n",
       "      <td>0.284901</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>leaf</td>\n",
       "      <td>→</td>\n",
       "      <td>paper</td>\n",
       "      <td>0.210082</td>\n",
       "      <td>0.341807</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.423160</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.203262</td>\n",
       "      <td>0.342544</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>look</td>\n",
       "      <td>→</td>\n",
       "      <td>appearance</td>\n",
       "      <td>0.200843</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.047516</td>\n",
       "      <td>0.049892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.262808</td>\n",
       "      <td>0.198636</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cool</td>\n",
       "      <td>→</td>\n",
       "      <td>calm</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.251869</td>\n",
       "      <td>0.095050</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.652084</td>\n",
       "      <td>0.530890</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  meaning1 shift_dir    meaning2  cos_sim_c1  cos_sim_c2  detected_dir_1.1  \\\n",
       "0     pope         →        fish   -0.248629   -0.337377              -1.0   \n",
       "1      hat         →         cap    0.899582    0.812439              -1.0   \n",
       "2     look         ↔        want    0.138726    0.219044               1.0   \n",
       "3  country         →      turkey    0.119608    0.361795               1.0   \n",
       "4     bird         —        comb    0.416332    0.285259              -1.0   \n",
       "5    stand         →       rebel    0.170375    0.152076              -1.0   \n",
       "6     draw         →      linger    0.164510    0.306689               1.0   \n",
       "7     leaf         →       paper    0.210082    0.341807               1.0   \n",
       "8     look         →  appearance    0.200843    0.258819               1.0   \n",
       "9     cool         →        calm    0.588342    0.627787               1.0   \n",
       "\n",
       "   cos_sim_c1_aligned  cos_sim_c2_aligned  detected_dir_1.2  \\\n",
       "0            0.157766            0.045846              -1.0   \n",
       "1            0.555460            0.322019              -1.0   \n",
       "2            0.303793            0.187486              -1.0   \n",
       "3            0.230626            0.148698              -1.0   \n",
       "4            0.563739            0.583203               1.0   \n",
       "5            0.224296            0.275593               1.0   \n",
       "6            0.183404            0.054489              -1.0   \n",
       "7            0.444800            0.423160              -1.0   \n",
       "8            0.047516            0.049892               1.0   \n",
       "9            0.251869            0.095050              -1.0   \n",
       "\n",
       "   cos_sim_c1_combined  cos_sim_c2_combined  detected_dir_1.3  \n",
       "0            -0.243716            -0.383739              -1.0  \n",
       "1             0.903475             0.792746              -1.0  \n",
       "2             0.137415             0.234502               1.0  \n",
       "3             0.059941             0.419437               1.0  \n",
       "4             0.383995             0.333703              -1.0  \n",
       "5             0.249531             0.217194              -1.0  \n",
       "6             0.218070             0.284901               1.0  \n",
       "7             0.203262             0.342544               1.0  \n",
       "8             0.262808             0.198636              -1.0  \n",
       "9             0.652084             0.530890              -1.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shift_res.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2.1: Average Embedding in Meaning Gloss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shifts in df_shift_res:  632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>cos_sim_c1_avg_emb</th>\n",
       "      <th>cos_sim_c2_avg_emb</th>\n",
       "      <th>detected_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pope]</td>\n",
       "      <td>→</td>\n",
       "      <td>[ruff, fish]</td>\n",
       "      <td>-0.193159</td>\n",
       "      <td>-0.287207</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hat]</td>\n",
       "      <td>→</td>\n",
       "      <td>[mushroom, cap]</td>\n",
       "      <td>0.886209</td>\n",
       "      <td>0.732821</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[search, look]</td>\n",
       "      <td>↔</td>\n",
       "      <td>[want]</td>\n",
       "      <td>0.193901</td>\n",
       "      <td>0.203343</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[country]</td>\n",
       "      <td>→</td>\n",
       "      <td>[turkey]</td>\n",
       "      <td>0.119608</td>\n",
       "      <td>0.361795</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[comb, bird]</td>\n",
       "      <td>—</td>\n",
       "      <td>[comb]</td>\n",
       "      <td>0.703312</td>\n",
       "      <td>0.648283</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[stand]</td>\n",
       "      <td>→</td>\n",
       "      <td>[revolt, rebel]</td>\n",
       "      <td>0.117913</td>\n",
       "      <td>0.211288</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[pull, draw]</td>\n",
       "      <td>→</td>\n",
       "      <td>[slow, linger]</td>\n",
       "      <td>0.178873</td>\n",
       "      <td>0.391186</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[leaf]</td>\n",
       "      <td>→</td>\n",
       "      <td>[sheet, paper]</td>\n",
       "      <td>0.397161</td>\n",
       "      <td>0.533569</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[see, look]</td>\n",
       "      <td>→</td>\n",
       "      <td>[appearance]</td>\n",
       "      <td>0.245025</td>\n",
       "      <td>0.263974</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[cool]</td>\n",
       "      <td>→</td>\n",
       "      <td>[calm]</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         meaning1 shift_dir         meaning2  cos_sim_c1_avg_emb  \\\n",
       "0          [pope]         →     [ruff, fish]           -0.193159   \n",
       "1           [hat]         →  [mushroom, cap]            0.886209   \n",
       "2  [search, look]         ↔           [want]            0.193901   \n",
       "3       [country]         →         [turkey]            0.119608   \n",
       "4    [comb, bird]         —           [comb]            0.703312   \n",
       "5         [stand]         →  [revolt, rebel]            0.117913   \n",
       "6    [pull, draw]         →   [slow, linger]            0.178873   \n",
       "7          [leaf]         →   [sheet, paper]            0.397161   \n",
       "8     [see, look]         →     [appearance]            0.245025   \n",
       "9          [cool]         →           [calm]            0.588342   \n",
       "\n",
       "   cos_sim_c2_avg_emb  detected_dir  \n",
       "0           -0.287207          -1.0  \n",
       "1            0.732821          -1.0  \n",
       "2            0.203343           1.0  \n",
       "3            0.361795           1.0  \n",
       "4            0.648283          -1.0  \n",
       "5            0.211288           1.0  \n",
       "6            0.391186           1.0  \n",
       "7            0.533569           1.0  \n",
       "8            0.263974           1.0  \n",
       "9            0.627787           1.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shift_res2 = pd.DataFrame(columns=['meaning1', 'shift_dir', 'meaning2'])\n",
    "num_shifts = 0\n",
    "\n",
    "# for each row in df\n",
    "for index, row in df.iterrows():\n",
    "    meaning1_words = []\n",
    "    meaning2_words = []\n",
    "    meaning1_avgs_wv = []\n",
    "    meaning2_avgs_wv = []\n",
    "    meaning1_avgs_wv2 = []\n",
    "    meaning2_avgs_wv2 = []\n",
    "    # for each word in meaning1\n",
    "    for word in row['meaning1']:\n",
    "        # if word is in the vocabulary\n",
    "        if word in wv and word in wv2:\n",
    "            meaning1_words.append(word)\n",
    "    # for each word in meaning2\n",
    "    for word in row['meaning2']:\n",
    "        # if word is in the vocabulary\n",
    "        if word in wv and word in wv2:\n",
    "            meaning2_words.append(word)\n",
    "\n",
    "    # if meaning1_word and meaning2_word are not empty\n",
    "    if len(meaning1_words) > 0 and len(meaning2_words) > 0:\n",
    "        # if meaning1_word and meaning2_word are not the same word\n",
    "        if meaning1_words != meaning2_words:\n",
    "            num_shifts += 1\n",
    "\n",
    "            meaning1_avgs_wv = np.mean(np.array([wv.get_vector(word) for word in meaning1_words]), axis=0)\n",
    "            meaning2_avgs_wv = np.mean(np.array([wv.get_vector(word) for word in meaning2_words]), axis=0)\n",
    "\n",
    "            meaning1_avgs_wv2 = np.mean(np.array([wv2.get_vector(word) for word in meaning1_words]), axis=0)\n",
    "            meaning2_avgs_wv2 = np.mean(np.array([wv2.get_vector(word) for word in meaning2_words]), axis=0)\n",
    "\n",
    "            # calculate cosine similarity between meaning1 and meaning2\n",
    "            cos_sim_c1 = cosine_similarity(meaning1_avgs_wv.reshape(1,-1), meaning2_avgs_wv.reshape(1,-1))[0][0]\n",
    "\n",
    "            cos_sim_c2 = cosine_similarity(meaning1_avgs_wv2.reshape(1,-1), meaning2_avgs_wv2.reshape(1,-1))[0][0]\n",
    "\n",
    "            # if cosine similarity of meaning1 and meaning2 in wv2 is greater than cosine similarity of meaning1 and meaning2 in wv\n",
    "            if cos_sim_c2 > cos_sim_c1:\n",
    "                shift_dir = 1.0\n",
    "            else:\n",
    "                shift_dir = -1.0\n",
    "\n",
    "            # add meaning1_words and meaning2_words, their average vectors, and cosine similarity to df_shift_res2\n",
    "            df_shift_res2 = df_shift_res2.append({'meaning1': meaning1_words, 'shift_dir': row['shift_dir'], 'meaning2': meaning2_words, \n",
    "                                                'cos_sim_c1_avg_emb': cos_sim_c1, 'cos_sim_c2_avg_emb': cos_sim_c2, 'detected_dir': shift_dir}, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"Number of shifts in df_shift_res: \", len(df_shift_res2))\n",
    "df_shift_res2.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly predicted shifts:  362\n",
      "Number of total shifts:  632\n",
      "Accuracy:  0.5727848101265823\n"
     ]
    }
   ],
   "source": [
    "# count # of correctly predicted shifts\n",
    "correct_shifts = 0\n",
    "for i in range(len(df_shift_res2)):\n",
    "    if df_shift_res2.iloc[i]['detected_dir'] == 1.0:\n",
    "        correct_shifts += 1\n",
    "\n",
    "print(\"Number of correctly predicted shifts: \", correct_shifts)\n",
    "print(\"Number of total shifts: \", len(df_shift_res2))\n",
    "print(\"Accuracy: \", correct_shifts / len(df_shift_res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2.2: Separate embeddings with alignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_shift_words_all = []\n",
    "\n",
    "# for each word in sem_shift_words\n",
    "for i in range(len(df_shift_res2)):\n",
    "    sem_shift_words_all.extend(df_shift_res2.iloc[i]['meaning1'])\n",
    "    sem_shift_words_all.extend(df_shift_res2.iloc[i]['meaning2'])\n",
    "\n",
    "wv_sem_shift_vecs_all = np.array([wv.get_vector(word) for word in sem_shift_words_all])\n",
    "wv2_sem_shift_vecs_all = np.array([wv2.get_vector(word) for word in sem_shift_words_all])\n",
    "\n",
    "wv_sem_shift_vecs_all_mu = np.mean(wv_sem_shift_vecs_all, axis=0)\n",
    "wv_sem_shift_vecs_all_centered = wv_sem_shift_vecs_all - wv_sem_shift_vecs_all_mu\n",
    "\n",
    "wv2_sem_shift_vecs_all_mu = np.mean(wv2_sem_shift_vecs_all, axis=0)\n",
    "wv2_sem_shift_vecs_all_centered = wv2_sem_shift_vecs_all - wv2_sem_shift_vecs_all_mu\n",
    "\n",
    "R, sca = orthogonal_procrustes(wv_sem_shift_vecs_all_centered, wv2_sem_shift_vecs_all_centered)\n",
    "scale = sca / np.square(norm(wv_sem_shift_vecs_all_centered))\n",
    "\n",
    "wv2_sem_shift_all_approx = np.dot(wv_sem_shift_vecs_all_centered, R) * scale + wv2_sem_shift_vecs_all_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>cos_sim_c1_avg_emb</th>\n",
       "      <th>cos_sim_c2_avg_emb</th>\n",
       "      <th>detected_dir</th>\n",
       "      <th>cos_sim_c1_aligned_all</th>\n",
       "      <th>cos_sim_c2_aligned_all</th>\n",
       "      <th>detected_dir_2.2</th>\n",
       "      <th>cos_sim_c1_combined_all</th>\n",
       "      <th>cos_sim_c2_combined_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pope]</td>\n",
       "      <td>→</td>\n",
       "      <td>[ruff, fish]</td>\n",
       "      <td>-0.193159</td>\n",
       "      <td>-0.287207</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.029504</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.225196</td>\n",
       "      <td>-0.383828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hat]</td>\n",
       "      <td>→</td>\n",
       "      <td>[mushroom, cap]</td>\n",
       "      <td>0.886209</td>\n",
       "      <td>0.732821</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.395676</td>\n",
       "      <td>0.084760</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.897048</td>\n",
       "      <td>0.716240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[search, look]</td>\n",
       "      <td>↔</td>\n",
       "      <td>[want]</td>\n",
       "      <td>0.193901</td>\n",
       "      <td>0.203343</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.359364</td>\n",
       "      <td>0.174429</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.187692</td>\n",
       "      <td>0.248006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[country]</td>\n",
       "      <td>→</td>\n",
       "      <td>[turkey]</td>\n",
       "      <td>0.119608</td>\n",
       "      <td>0.361795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.418272</td>\n",
       "      <td>0.323525</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.083844</td>\n",
       "      <td>0.342028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[comb, bird]</td>\n",
       "      <td>—</td>\n",
       "      <td>[comb]</td>\n",
       "      <td>0.703312</td>\n",
       "      <td>0.648283</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.500387</td>\n",
       "      <td>0.363287</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.659231</td>\n",
       "      <td>0.633424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[stand]</td>\n",
       "      <td>→</td>\n",
       "      <td>[revolt, rebel]</td>\n",
       "      <td>0.117913</td>\n",
       "      <td>0.211288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.568120</td>\n",
       "      <td>0.126827</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.219444</td>\n",
       "      <td>0.138628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[pull, draw]</td>\n",
       "      <td>→</td>\n",
       "      <td>[slow, linger]</td>\n",
       "      <td>0.178873</td>\n",
       "      <td>0.391186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.082797</td>\n",
       "      <td>0.120619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.195974</td>\n",
       "      <td>0.361573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[leaf]</td>\n",
       "      <td>→</td>\n",
       "      <td>[sheet, paper]</td>\n",
       "      <td>0.397161</td>\n",
       "      <td>0.533569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.616859</td>\n",
       "      <td>0.647212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.384226</td>\n",
       "      <td>0.527084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[see, look]</td>\n",
       "      <td>→</td>\n",
       "      <td>[appearance]</td>\n",
       "      <td>0.245025</td>\n",
       "      <td>0.263974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.523005</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.312477</td>\n",
       "      <td>0.203783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[cool]</td>\n",
       "      <td>→</td>\n",
       "      <td>[calm]</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.591769</td>\n",
       "      <td>0.377467</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.648213</td>\n",
       "      <td>0.606192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         meaning1 shift_dir         meaning2  cos_sim_c1_avg_emb  \\\n",
       "0          [pope]         →     [ruff, fish]           -0.193159   \n",
       "1           [hat]         →  [mushroom, cap]            0.886209   \n",
       "2  [search, look]         ↔           [want]            0.193901   \n",
       "3       [country]         →         [turkey]            0.119608   \n",
       "4    [comb, bird]         —           [comb]            0.703312   \n",
       "5         [stand]         →  [revolt, rebel]            0.117913   \n",
       "6    [pull, draw]         →   [slow, linger]            0.178873   \n",
       "7          [leaf]         →   [sheet, paper]            0.397161   \n",
       "8     [see, look]         →     [appearance]            0.245025   \n",
       "9          [cool]         →           [calm]            0.588342   \n",
       "\n",
       "   cos_sim_c2_avg_emb  detected_dir  cos_sim_c1_aligned_all  \\\n",
       "0           -0.287207          -1.0                0.029504   \n",
       "1            0.732821          -1.0                0.395676   \n",
       "2            0.203343           1.0                0.359364   \n",
       "3            0.361795           1.0                0.418272   \n",
       "4            0.648283          -1.0                0.500387   \n",
       "5            0.211288           1.0                0.568120   \n",
       "6            0.391186           1.0                0.082797   \n",
       "7            0.533569           1.0                0.616859   \n",
       "8            0.263974           1.0                0.567600   \n",
       "9            0.627787           1.0                0.591769   \n",
       "\n",
       "   cos_sim_c2_aligned_all  detected_dir_2.2  cos_sim_c1_combined_all  \\\n",
       "0                0.124642               1.0                -0.225196   \n",
       "1                0.084760              -1.0                 0.897048   \n",
       "2                0.174429              -1.0                 0.187692   \n",
       "3                0.323525              -1.0                 0.083844   \n",
       "4                0.363287              -1.0                 0.659231   \n",
       "5                0.126827              -1.0                 0.219444   \n",
       "6                0.120619               1.0                 0.195974   \n",
       "7                0.647212               1.0                 0.384226   \n",
       "8                0.523005              -1.0                 0.312477   \n",
       "9                0.377467              -1.0                 0.648213   \n",
       "\n",
       "   cos_sim_c2_combined_all  \n",
       "0                -0.383828  \n",
       "1                 0.716240  \n",
       "2                 0.248006  \n",
       "3                 0.342028  \n",
       "4                 0.633424  \n",
       "5                 0.138628  \n",
       "6                 0.361573  \n",
       "7                 0.527084  \n",
       "8                 0.203783  \n",
       "9                 0.606192  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_res_all_words_aligned = [] \n",
    "meaning1_index = 0\n",
    "meaning2_index = 0\n",
    "# count how many strings are in meaning1\n",
    "for i in range(len(df_shift_res2)):\n",
    "    meaning2_index += len(df_shift_res2.iloc[i]['meaning1'])\n",
    "\n",
    "for i in range(len(df_shift_res2)):\n",
    "    meaning1_words = df_shift_res2.iloc[i]['meaning1']\n",
    "    meaning1_wv_c1 = wv2_sem_shift_all_approx[meaning1_index:meaning1_index+len(meaning1_words)]\n",
    "    meaning1_wv_c2 = wv2_sem_shift_vecs_all[meaning1_index:meaning1_index+len(meaning1_words)]\n",
    "    meaning1_index += len(meaning1_words)\n",
    "    meaning1_avgs_wv_c1 = np.mean(meaning1_wv_c1, axis=0)\n",
    "    meaning1_avgs_wv_c2 = np.mean(meaning1_wv_c2, axis=0)\n",
    "\n",
    "    meaning2_words = df_shift_res2.iloc[i]['meaning2']\n",
    "    meaning2_wv_c1 = wv2_sem_shift_all_approx[meaning2_index:meaning2_index+len(meaning2_words)]\n",
    "    meaning2_wv_c2 = wv2_sem_shift_vecs_all[meaning2_index:meaning2_index+len(meaning2_words)]\n",
    "    meaning2_index += len(meaning2_words)\n",
    "    meaning2_avgs_wv_c1 = np.mean(meaning2_wv_c1, axis=0)\n",
    "    meaning2_avgs_wv_c2 = np.mean(meaning2_wv_c2, axis=0)\n",
    "\n",
    "    cos_sim_c1 = cosine_similarity(meaning1_avgs_wv_c1.reshape(1,-1), meaning2_avgs_wv_c1.reshape(1,-1))[0][0]\n",
    "    cos_sim_c2 = cosine_similarity(meaning1_avgs_wv_c2.reshape(1,-1), meaning2_avgs_wv_c2.reshape(1,-1))[0][0]\n",
    "\n",
    "    detection_res_all_words_aligned.append([cos_sim_c1, cos_sim_c2, np.sign(cos_sim_c2 - cos_sim_c1)])\n",
    "\n",
    "\n",
    "# add the cosine similarity of meaning1 and meaning2 in df_shift_res to df_shift_res\n",
    "df_shift_res2['cos_sim_c1_aligned_all'] = [detection_res_all_words_aligned[i][0] for i in range(len(detection_res_all_words_aligned))]\n",
    "df_shift_res2['cos_sim_c2_aligned_all'] = [detection_res_all_words_aligned[i][1] for i in range(len(detection_res_all_words_aligned))]\n",
    "df_shift_res2['detected_dir_2.2'] = [detection_res_all_words_aligned[i][2] for i in range(len(detection_res_all_words_aligned))]\n",
    "\n",
    "df_shift_res2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly predicted shifts:  152\n",
      "Number of total shifts:  632\n",
      "Accuracy:  0.24050632911392406\n"
     ]
    }
   ],
   "source": [
    "# count # of correctly predicted shifts\n",
    "correct_shifts = 0\n",
    "for i in range(len(df_shift_res2)):\n",
    "    if df_shift_res2.iloc[i]['detected_dir_2.2'] == 1.0:\n",
    "        correct_shifts += 1\n",
    "\n",
    "print(\"Number of correctly predicted shifts: \", correct_shifts)\n",
    "print(\"Number of total shifts: \", len(df_shift_res2))\n",
    "print(\"Accuracy: \", correct_shifts / len(df_shift_res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2.3: Combined word2vec embedding for all shift words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store corpus1 and corpus2 lines together\n",
    "c1_c2_lines = []\n",
    "\n",
    "with open(\"test_data_public/english/corpus1/lemma/ccoha1.txt\") as file:\n",
    "    c1_lines = [line.rstrip().split() for line in file]\n",
    "    c1_c2_lines.extend(c1_lines)\n",
    "\n",
    "with open(\"test_data_public/english/corpus2/lemma/ccoha2.txt\") as file:\n",
    "    c2_lines = [line.rstrip().split() for line in file]\n",
    "    for line in c2_lines:\n",
    "        for word in line:\n",
    "            if word in sem_shift_words_all:\n",
    "                line[line.index(word)] = word + '_'\n",
    "    c1_c2_lines.extend(c2_lines)\n",
    "\n",
    "# Train combined model\n",
    "model_combined = Word2Vec(c1_c2_lines, vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors_combined_sem_shift_all = model_combined.wv\n",
    "word_vectors_combined_sem_shift_all.save(\"test_data_public/english/ccoha_combined_sem_shift_all.wv\")\n",
    "\n",
    "wv_combined_sem_shift_all = KeyedVectors.load(\"test_data_public/english/ccoha_combined_sem_shift_all.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meaning1</th>\n",
       "      <th>shift_dir</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>cos_sim_c1_avg_emb</th>\n",
       "      <th>cos_sim_c2_avg_emb</th>\n",
       "      <th>detected_dir</th>\n",
       "      <th>cos_sim_c1_aligned_all</th>\n",
       "      <th>cos_sim_c2_aligned_all</th>\n",
       "      <th>detected_dir_2.2</th>\n",
       "      <th>cos_sim_c1_combined_all</th>\n",
       "      <th>cos_sim_c2_combined_all</th>\n",
       "      <th>detected_dir_2.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pope]</td>\n",
       "      <td>→</td>\n",
       "      <td>[ruff, fish]</td>\n",
       "      <td>-0.193159</td>\n",
       "      <td>-0.287207</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.029504</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.225196</td>\n",
       "      <td>-0.383828</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hat]</td>\n",
       "      <td>→</td>\n",
       "      <td>[mushroom, cap]</td>\n",
       "      <td>0.886209</td>\n",
       "      <td>0.732821</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.395676</td>\n",
       "      <td>0.084760</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.897048</td>\n",
       "      <td>0.716240</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[search, look]</td>\n",
       "      <td>↔</td>\n",
       "      <td>[want]</td>\n",
       "      <td>0.193901</td>\n",
       "      <td>0.203343</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.359364</td>\n",
       "      <td>0.174429</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.187692</td>\n",
       "      <td>0.248006</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[country]</td>\n",
       "      <td>→</td>\n",
       "      <td>[turkey]</td>\n",
       "      <td>0.119608</td>\n",
       "      <td>0.361795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.418272</td>\n",
       "      <td>0.323525</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.083844</td>\n",
       "      <td>0.342028</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[comb, bird]</td>\n",
       "      <td>—</td>\n",
       "      <td>[comb]</td>\n",
       "      <td>0.703312</td>\n",
       "      <td>0.648283</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.500387</td>\n",
       "      <td>0.363287</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.659231</td>\n",
       "      <td>0.633424</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[stand]</td>\n",
       "      <td>→</td>\n",
       "      <td>[revolt, rebel]</td>\n",
       "      <td>0.117913</td>\n",
       "      <td>0.211288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.568120</td>\n",
       "      <td>0.126827</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.219444</td>\n",
       "      <td>0.138628</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[pull, draw]</td>\n",
       "      <td>→</td>\n",
       "      <td>[slow, linger]</td>\n",
       "      <td>0.178873</td>\n",
       "      <td>0.391186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.082797</td>\n",
       "      <td>0.120619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.195974</td>\n",
       "      <td>0.361573</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[leaf]</td>\n",
       "      <td>→</td>\n",
       "      <td>[sheet, paper]</td>\n",
       "      <td>0.397161</td>\n",
       "      <td>0.533569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.616859</td>\n",
       "      <td>0.647212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.384226</td>\n",
       "      <td>0.527084</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[see, look]</td>\n",
       "      <td>→</td>\n",
       "      <td>[appearance]</td>\n",
       "      <td>0.245025</td>\n",
       "      <td>0.263974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.523005</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.312477</td>\n",
       "      <td>0.203783</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[cool]</td>\n",
       "      <td>→</td>\n",
       "      <td>[calm]</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.627787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.591769</td>\n",
       "      <td>0.377467</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.648213</td>\n",
       "      <td>0.606192</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         meaning1 shift_dir         meaning2  cos_sim_c1_avg_emb  \\\n",
       "0          [pope]         →     [ruff, fish]           -0.193159   \n",
       "1           [hat]         →  [mushroom, cap]            0.886209   \n",
       "2  [search, look]         ↔           [want]            0.193901   \n",
       "3       [country]         →         [turkey]            0.119608   \n",
       "4    [comb, bird]         —           [comb]            0.703312   \n",
       "5         [stand]         →  [revolt, rebel]            0.117913   \n",
       "6    [pull, draw]         →   [slow, linger]            0.178873   \n",
       "7          [leaf]         →   [sheet, paper]            0.397161   \n",
       "8     [see, look]         →     [appearance]            0.245025   \n",
       "9          [cool]         →           [calm]            0.588342   \n",
       "\n",
       "   cos_sim_c2_avg_emb  detected_dir  cos_sim_c1_aligned_all  \\\n",
       "0           -0.287207          -1.0                0.029504   \n",
       "1            0.732821          -1.0                0.395676   \n",
       "2            0.203343           1.0                0.359364   \n",
       "3            0.361795           1.0                0.418272   \n",
       "4            0.648283          -1.0                0.500387   \n",
       "5            0.211288           1.0                0.568120   \n",
       "6            0.391186           1.0                0.082797   \n",
       "7            0.533569           1.0                0.616859   \n",
       "8            0.263974           1.0                0.567600   \n",
       "9            0.627787           1.0                0.591769   \n",
       "\n",
       "   cos_sim_c2_aligned_all  detected_dir_2.2  cos_sim_c1_combined_all  \\\n",
       "0                0.124642               1.0                -0.225196   \n",
       "1                0.084760              -1.0                 0.897048   \n",
       "2                0.174429              -1.0                 0.187692   \n",
       "3                0.323525              -1.0                 0.083844   \n",
       "4                0.363287              -1.0                 0.659231   \n",
       "5                0.126827              -1.0                 0.219444   \n",
       "6                0.120619               1.0                 0.195974   \n",
       "7                0.647212               1.0                 0.384226   \n",
       "8                0.523005              -1.0                 0.312477   \n",
       "9                0.377467              -1.0                 0.648213   \n",
       "\n",
       "   cos_sim_c2_combined_all  detected_dir_2.3  \n",
       "0                -0.383828              -1.0  \n",
       "1                 0.716240              -1.0  \n",
       "2                 0.248006               1.0  \n",
       "3                 0.342028               1.0  \n",
       "4                 0.633424              -1.0  \n",
       "5                 0.138628              -1.0  \n",
       "6                 0.361573               1.0  \n",
       "7                 0.527084               1.0  \n",
       "8                 0.203783              -1.0  \n",
       "9                 0.606192              -1.0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_res_all_words_combined = [] \n",
    "\n",
    "for i in range(len(df_shift_res2)):\n",
    "    meaning1_words = df_shift_res2.iloc[i]['meaning1']\n",
    "    meaning2_words = df_shift_res2.iloc[i]['meaning2']\n",
    "\n",
    "    meaning1_avgs_wv = np.mean(np.array([wv_combined_sem_shift_all.get_vector(word) for word in meaning1_words]), axis=0)\n",
    "    meaning2_avgs_wv = np.mean(np.array([wv_combined_sem_shift_all.get_vector(word) for word in meaning2_words]), axis=0)\n",
    "\n",
    "    meaning1_avgs_wv2 = np.mean(np.array([wv_combined_sem_shift_all.get_vector(word + \"_\") for word in meaning1_words]), axis=0)\n",
    "    meaning2_avgs_wv2 = np.mean(np.array([wv_combined_sem_shift_all.get_vector(word + \"_\") for word in meaning2_words]), axis=0)\n",
    "\n",
    "    # calculate cosine similarity between meaning1 and meaning2\n",
    "    cos_sim_c1 = cosine_similarity(meaning1_avgs_wv.reshape(1,-1), meaning2_avgs_wv.reshape(1,-1))[0][0]\n",
    "    cos_sim_c2 = cosine_similarity(meaning1_avgs_wv2.reshape(1,-1), meaning2_avgs_wv2.reshape(1,-1))[0][0]\n",
    "\n",
    "    detection_res_all_words_combined.append([cos_sim_c1, cos_sim_c2, np.sign(cos_sim_c2 - cos_sim_c1)])\n",
    "\n",
    "df_shift_res2['cos_sim_c1_combined_all'] = [detection_res_all_words_combined[i][0] for i in range(len(detection_res_all_words_combined))]\n",
    "df_shift_res2['cos_sim_c2_combined_all'] = [detection_res_all_words_combined[i][1] for i in range(len(detection_res_all_words_combined))]\n",
    "df_shift_res2['detected_dir_2.3'] = [detection_res_all_words_combined[i][2] for i in range(len(detection_res_all_words_combined))]\n",
    "\n",
    "df_shift_res2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly predicted shifts:  298\n",
      "Number of total shifts:  632\n",
      "Accuracy:  0.47151898734177217\n"
     ]
    }
   ],
   "source": [
    "# count # of correctly predicted shifts\n",
    "correct_shifts = 0\n",
    "for i in range(len(df_shift_res2)):\n",
    "    if df_shift_res2.iloc[i]['detected_dir_2.3'] == 1.0:\n",
    "        correct_shifts += 1\n",
    "\n",
    "print(\"Number of correctly predicted shifts: \", correct_shifts)\n",
    "print(\"Number of total shifts: \", len(df_shift_res2))\n",
    "print(\"Accuracy: \", correct_shifts / len(df_shift_res2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
