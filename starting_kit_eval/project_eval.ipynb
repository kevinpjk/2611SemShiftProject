{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SemEval2020 Task 1 Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Train two models for two time points and align them using Orthogonal Procrustes\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Training word2vec model on english practice corpus 1\n",
    "model = Word2Vec(corpus_file=\"test_data_public/english/corpus1/lemma/ccoha1.txt\", vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "model.save(\"test_data_public/english/corpus1/lemma/ccoha1.model\")\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"test_data_public/english/corpus1/lemma/ccoha1.wv\")\n",
    "\n",
    "wv = KeyedVectors.load(\"test_data_public/english/corpus1/lemma/ccoha1.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train second word2vec model using corpus2 lines\n",
    "model2 = Word2Vec(corpus_file=\"test_data_public/english/corpus2/lemma/ccoha2.txt\", vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "model2.save(\"test_data_public/english/corpus2/lemma/ccoha2.model\")\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors2 = model2.wv\n",
    "word_vectors2.save(\"test_data_public/english/corpus2/lemma/ccoha2.wv\")\n",
    "\n",
    "wv2 = KeyedVectors.load(\"test_data_public/english/corpus2/lemma/ccoha2.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attack_nn', 'bag_nn', 'ball_nn', 'bit_nn', 'chairman_nn', 'circle_vb', 'contemplation_nn', 'donkey_nn', 'edge_nn', 'face_nn', 'fiction_nn', 'gas_nn', 'graft_nn', 'head_nn', 'land_nn', 'lane_nn', 'lass_nn', 'multitude_nn', 'ounce_nn', 'part_nn', 'pin_vb', 'plane_nn', 'player_nn', 'prop_nn', 'quilt_nn', 'rag_nn', 'record_nn', 'relationship_nn', 'risk_nn', 'savage_nn', 'stab_nn', 'stroke_vb', 'thump_nn', 'tip_vb', 'tree_nn', 'twist_nn', 'word_nn']\n"
     ]
    }
   ],
   "source": [
    "# Load target words from targets.txt\n",
    "target_words = []\n",
    "with open(\"test_data_public/english/targets.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        target_words.append(line.strip())\n",
    "\n",
    "print(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align wv and wv2 using Orthogonal Procrustes\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# print(wv.get_normed_vectors(), wv2.get_normed_vectors())\n",
    "\n",
    "# Get vectors for target words\n",
    "wv_target_vectors = np.array([wv.get_vector(word) for word in target_words])\n",
    "wv2_target_vectors = np.array([wv2.get_vector(word) for word in target_words])\n",
    "\n",
    "wv_target_vectors_mu = wv_target_vectors.mean(axis=0)\n",
    "wv_target_vectors_centered = wv_target_vectors - wv_target_vectors_mu\n",
    "\n",
    "wv2_target_vectors_mu = wv2_target_vectors.mean(axis=0)\n",
    "wv2_target_vectors_centered = wv2_target_vectors - wv2_target_vectors_mu\n",
    "\n",
    "R, sca = orthogonal_procrustes(wv_target_vectors_centered, wv2_target_vectors_centered)\n",
    "# print(R, sca)\n",
    "scale = sca / np.square(norm(wv_target_vectors_centered))\n",
    "\n",
    "wv2_target_vectors_approx = scale * np.dot(wv_target_vectors_centered, R) + wv2_target_vectors_mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy (align):  0.5945945945945946\n"
     ]
    }
   ],
   "source": [
    "# Task 1 - Compute target words cosine similarity between aligned vectors and wv2 vectors\n",
    "task1_similarities = []\n",
    "for i, word in enumerate(target_words):\n",
    "    # print(word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1)))\n",
    "    task1_similarities.append([word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1))[0][0]])\n",
    "\n",
    "# print(task1_similarities)\n",
    "\n",
    "# Load truth file\n",
    "task1_truth = []\n",
    "with open(\"test_data_public/english/truth/binary.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        task1_truth.append(line.strip().split())\n",
    "\n",
    "# For each list in task1_similarities, check if value is less than 0.9, assign 1 if true, 0 if false\n",
    "task1_res = []\n",
    "for i, word in enumerate(task1_similarities):\n",
    "    if word[1] < 0.9:\n",
    "        task1_res.append([word[0], 1])\n",
    "    else:\n",
    "        task1_res.append([word[0], 0])\n",
    "\n",
    "# Compare task1_res and task1_truth\n",
    "task1_correct = 0\n",
    "for i, word in enumerate(task1_res):\n",
    "    if word[1] == int(task1_truth[i][1]):\n",
    "        task1_correct += 1\n",
    "\n",
    "print(\"Task 1 accuracy (align): \", task1_correct/len(task1_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Train a combined model on both corpora (target words in corpus2 are changed to target_word_)\n",
    "\n",
    "# Store corpus1 and corpus2 lines together\n",
    "c1_c2_lines = []\n",
    "\n",
    "with open(\"test_data_public/english/corpus1/lemma/ccoha1.txt\") as file:\n",
    "    c1_lines = [line.rstrip().split() for line in file]\n",
    "    c1_c2_lines.extend(c1_lines)\n",
    "\n",
    "with open(\"test_data_public/english/corpus2/lemma/ccoha2.txt\") as file:\n",
    "    c2_lines = [line.rstrip().split() for line in file]\n",
    "    for line in c2_lines:\n",
    "        for word in line:\n",
    "            if word in target_words:\n",
    "                line[line.index(word)] = word + '_'\n",
    "    c1_c2_lines.extend(c2_lines)\n",
    "\n",
    "# Train combined model\n",
    "model_combined = Word2Vec(c1_c2_lines, vector_size=300, window=10, min_count=1, workers=4, negative=5)\n",
    "\n",
    "# Saving wordvectors\n",
    "word_vectors_combined = model_combined.wv\n",
    "word_vectors_combined.save(\"test_data_public/english/ccoha_combined.wv\")\n",
    "\n",
    "wv_combined = KeyedVectors.load(\"test_data_public/english/ccoha_combined.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy (combined):  0.6756756756756757\n"
     ]
    }
   ],
   "source": [
    "# Task 1 - Compute target words cosine distance in combined word vector\n",
    "task1_similarities_combined = []\n",
    "for i, word in enumerate(target_words):\n",
    "    # print(word, cosine_similarity(wv2_target_vectors_approx[i].reshape(1,-1), wv2_target_vectors[i].reshape(1,-1)))\n",
    "    task1_similarities_combined.append([word, cosine_similarity(wv_combined.get_vector(word).reshape(1,-1), wv_combined.get_vector(word + '_').reshape(1,-1))[0][0]])\n",
    "\n",
    "# print(task1_similarities_combined)\n",
    "\n",
    "# Compare task1_similarities_combined and task1_truth\n",
    "task1_combined_res = []\n",
    "for i, word in enumerate(task1_similarities_combined):\n",
    "    if word[1] < 0.65:\n",
    "        task1_combined_res.append([word[0], 1])\n",
    "    else:\n",
    "        task1_combined_res.append([word[0], 0])\n",
    "\n",
    "task1_combined_correct = 0\n",
    "for i, word in enumerate(task1_combined_res):\n",
    "    if word[1] == int(task1_truth[i][1]):\n",
    "        task1_combined_correct += 1\n",
    "\n",
    "print(\"Task 1 accuracy (combined): \", task1_combined_correct/len(task1_combined_res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30922a6c5955a1d7417cdb8440e3c5be4b48d1d766725ea5fb70086fab274955"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
